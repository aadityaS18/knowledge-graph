# -*- coding: utf-8 -*-
"""RAW WITH KNOWLEDGE GRAPH

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11mRCdOhlTqwVAuiCsAF2QScJbBsFaJgL
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install  --upgrade --quiet langchain langchain-community langchain-openai langchain-experimental wikipedia neo4j tiktoken yfiles_jupyter_graphs

from google.colab import userdata
from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain.text_splitter import TokenTextSplitter
from yfiles_jupyter_graphs import GraphWidget
from langchain_community.vectorstores import Neo4jVector
from langchain_openai import OpenAIEmbeddings
from neo4j import GraphDatabase
from langchain_community.graphs import Neo4jGraph
from langchain.document_loaders import WikipediaLoader
from langchain_core.pydantic_v1 import BaseModel,Field
from langchain.schema.runnable import RunnableBranch, RunnableLambda, RunnablePassthrough
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage, AIMessage
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
import os

open_api_key = ""
!export OPEN_API_KEY=''

os.environ["NEO4J_URI"] = "neo4j+s://fd919b29.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "nhn2RV52iGrQdn3MfYbhy80Jp_QrzUc3nuLb9XOB4TI"

graph = Neo4jGraph(
    url=os.environ.get("NEO4J_URI"),
    username=os.environ.get("NEO4J_USERNAME"),
    password=os.environ.get("NEO4J_PASSWORD")
)
raw_documents = WikipediaLoader(query="Elizabeth I").load()

len(raw_documents)

raw_documents[:3]

from langchain.text_splitter import TokenTextSplitter
text_splitter=TokenTextSplitter(chunk_size=512,chunk_overlap=24)
documents=text_splitter.split_documents(raw_documents[:3])

from langchain_openai import ChatOpenAI
llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-0125", openai_api_key=open_api_key)

from langchain_experimental.graph_transformers import LLMGraphTransformer
llm_transformer=LLMGraphTransformer(llm=llm)

"""# New Section"""

default_cypher="MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50"

from yfiles_jupyter_graphs import GraphWidget
from neo4j import GraphDatabase

try:
  import google.colab
  from google.colab import output
  output.enable_custom_widget_manager()
except:
  pass

def showGraph(cypher: str=default_cypher):
  driver=GraphDatabase.driver(
      uri=os.environ["NEO4J_URI"],
      auth=(os.environ["NEO4J_USERNAME"],
            os.environ["NEO4J_PASSWORD"]))
  session=driver.session()
  widget=GraphWidget(graph=session.run(cypher).graph())
  widget.node_label_mapping="id"
  display(widget)
  return widget

showGraph()

from typing import Tuple,List,Optional

from langchain_community.vectorstores import Neo4jVector
from langchain_openai import OpenAIEmbeddings

openai_api_key = ''

embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)

# Specify a new index name
vector_index = Neo4jVector.from_existing_graph(
    embedding,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    index_name="my_new_vector_index"
)

from langchain_core.pydantic_v1 import BaseModel,Field
class Entities(BaseModel):
  names:List[str]=Field(
      description=" All the person,organisations,or business entities ""appear in the text",

  )

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

prompt=ChatPromptTemplate.from_messages(
    [
        (
        "system",
        "You are extracting organistaion and person entities from the text.",
        ),
        (
            "human",
            "Use the format to extract the infomation from following"
            "input: {question}",
        )



    ]
)

entity_chain=prompt | llm.with_structured_output(Entities)

entity_chain.invoke({"question":"Where was Amelia Earhart born?"}).names

from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars

graph.query("CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR(e:__Entity__)ON EACH [e.id]")

def generate_full_text_query(input: str)-> str:
  full_text_query=""
  words=[el for el in remove_lucene_chars(input).split()if el]
  if words: # Check if words is not empty
    for word in words[:-1]:
      full_text_query+=f"{word}~2 AND"
    full_text_query+=f"{words[-1]}~2" # Use words[-1] instead of word[-1]
  return full_text_query.strip()

def structured_retriever(question:str)-> str:
  result=""
  entities=entity_chain.invoke({"question":question})
  for entity in entities.names:
    response=graph.query(
        """CALL db.index.fulltext.queryNodes('entity',$query,{limit:2})
        YIELD node,score
        CALL
          {
          WITH node
          MATCH(node)-[r:!MENTIONS]->(neighbor)
          RETURN node.id+ "-"+type(r)+" ->" +node.id AS OUTPUT
          UNION ALL
          WITH node
           MATCH (node)<-[r:!MENTIONS]-(neighbor)
          RETURN neighbor.id +" -"+ type(r)+"->"+node.id AS OUTPUT
        }
        RETURN OUTPUT LIMIT 50
        """,
        {"query":generate_full_text_query(entity)},




    )

    result+="\n".join([el["output"]for el in response])
  return result

print(structured_retriever("Who is ElizabethI ?"))

def retriever(question:str):
  print(f"search query:{question}")
  structured_data=structured_retriever(question)
  unstructured_data=[el.page_content for el in vector_index.similarity_search(question)]
  final_data=f"""STRUCTURED DATA:
  {structured_data}
  Unstructured data:
  {"#Document".join(unstructured_data)}
  """
  return final_data

_template="""Give the following conversation and a follow up question repharase in its original language.
Chat History:{chat_history}
Follow Up Input:{question}
Standalone question:"""

CONDENSE_QUESTION_PROMPT=PromptTemplate.from_template(_template)

def _format_chat_history(chat_history:List[Tuple[str,str]])->List:
  buffer=[]
  for human,ai in chat_history:
    buffer.append(HumanMessage(content=human))
    buffer.append(AIMessage(content=ai))

  return buffer

_search_query=RunnableBranch(
    (
        RunnableLambda(lambda x:bool(x.get("chat_history"))).with_config(
            run_name="Has ChatHistoryCheck"
        ),
        RunnablePassthrough.assign(
            chat_history=lambda x: _format_chat_history(x["chat_history"])

        )
        |CONDENSE_QUESTION_PROMPT
        |ChatOpenAI(temperature=0,openai_api_key = '')
        | StrOutputParser(),
    ),
    RunnableLambda(lambda x:x["question"]),
)

template="""Answer the question based on the context:
{context}
Question:{question}
Use natural language to answer the question
Answer:"""

prompt=ChatPromptTemplate.from_template(template)

from langchain_core.runnables import RunnableParallel

chain=(
    RunnableParallel(
        {
            "context":_search_query | retriever,
            "question":RunnablePassthrough(),

        }
    )
    | prompt
    |llm
    |StrOutputParser()
)

chain.invoke({"question":"Which house did Elizabeth I live in?"})

chain.invoke({"question":"Where was Elizabeth I born?"})

chain.invoke({"question":"When did  Elizabeth I crowned as queen?"})

chain.invoke({"question":"When did  Elizabeth I died?"})
